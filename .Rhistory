} else {
# Use Random Effect model if corellation does not exists
print("Random Effect Model:")
return(random_model)
}
}
})
# Step 4: Pool the results
pooled_results <- testEstimates(model_list)
# Return the pooled results summary
return(summary(pooled_results))
}
# Apply the function to each dataset and store results
analyze_mitml_bal_mcar_50 <- Analyze_mitml_Bal(mitml_bal_mcar_50)
analyze_mitml_bal_mcar_30 <- Analyze_mitml_Bal(mitml_bal_mcar_30)
analyze_mitml_bal_mcar_10 <- Analyze_mitml_Bal(mitml_bal_mcar_10)
analyze_mitml_bal_mar_50 <- Analyze_mitml_Bal(mitml_bal_mar_50)
analyze_mitml_bal_mar_30 <- Analyze_mitml_Bal(mitml_bal_mar_30)
analyze_mitml_bal_mar_10 <- Analyze_mitml_Bal(mitml_bal_mar_10)
analyze_mitml_bal_mnar_50 <- Analyze_mitml_Bal(mitml_bal_mnar_50)
analyze_mitml_bal_mnar_30 <- Analyze_mitml_Bal(mitml_bal_mnar_30)
analyze_mitml_bal_mnar_10 <- Analyze_mitml_Bal(mitml_bal_mnar_10)
## Unbalanced Panel ##
#####################
# Convert the data frame to a panel data frame
pdata_unbal <- pdata.frame(unbalanced_panel_data, index = c("ID", "Year"))
# Estimate the fixed effects model
fe_model <- plm(Income ~ Year + Education + Age, data = pdata_unbal, model = "within")
# Estimate the random effects model
re_model <- plm(Income ~ Year + Education + Age, data = pdata_unbal, model = "random")
# Perform the Hausman test to compare the fixed and random effects models
hausman_test <- phtest(fe_model, re_model)
# Print the results of the Hausman test
print(hausman_test)
# p-value is 2.2e-16, which is < 0.05. So null hypothesis can be rejected.
# implying that the fixed effects in Education and Age is more appropriate.
# Function to impute data for unbalanced panels
Data_Imputation_mitml_Unbal <- function(panel_data) {
# Prepare the data by ungrouping and selecting relevant columns
panel_data <- panel_data %>%
ungroup()
selected_data <- as.data.frame(panel_data[c("ID", "Year", "Education", "Age", "Income")])
# Define the type vector and assign column names
type <- c(0, -2, 2, 2, 1)
names(type) <- colnames(selected_data)
# Impute missing data
imputed_data <- panImpute(selected_data, type = type, n.burn = 1000, n.iter = 100, m = 3)
# Extract imputed datasets
imputed_list <- mitmlComplete(imputed_data, print = "all")
return(imputed_list)
}
# Apply the function to each dataset
mitml_unbal_mcar_50 <- Data_Imputation_mitml_Unbal(unbalanced_panel_data_mcar_50)
mitml_unbal_mcar_30 <- Data_Imputation_mitml_Unbal(unbalanced_panel_data_mcar_30)
mitml_unbal_mcar_10 <- Data_Imputation_mitml_Unbal(unbalanced_panel_data_mcar_10)
mitml_unbal_mar_50 <- Data_Imputation_mitml_Unbal(unbalanced_panel_data_mar_50)
mitml_unbal_mar_30 <- Data_Imputation_mitml_Unbal(unbalanced_panel_data_mar_30)
mitml_unbal_mar_10 <- Data_Imputation_mitml_Unbal(unbalanced_panel_data_mar_10)
mitml_unbal_mnar_50 <- Data_Imputation_mitml_Unbal(unbalanced_panel_data_mnar_50)
mitml_unbal_mnar_30 <- Data_Imputation_mitml_Unbal(unbalanced_panel_data_mnar_30)
mitml_unbal_mnar_10 <- Data_Imputation_mitml_Unbal(unbalanced_panel_data_mnar_10)
# Function to analyze imputed data and extract coefficients for unbalanced panels
Analyze_mitml_Unbal <- function(imputed_list) {
# Step 2: Perform Breusch-Pagan test to check for a panel effect
breusch_pagan_results <- lapply(imputed_list, function(x) {
pdata <- pdata.frame(x, index = c("ID", "Year"))
bp_test <- plmtest(plm(Income ~ Education + Age, data = pdata, model = "pooling"), type = "bp")
return(bp_test$p.value)
})
# Step 3: Based on Breusch-Pagan test, perform the appropriate regression
model_list <- lapply(1:length(imputed_list), function(i) {
pdata <- pdata.frame(imputed_list[[i]], index = c("ID", "Year"))
if (breusch_pagan_results[[i]] > 0.05) {
# Pooled OLS for no panel effect
return(plm(Income ~ Year + Education + Age, data = pdata, model = "pooling"))
} else {
# Hausman test if panel effect exists
random_model <- plm(Income ~ Year + Education + Age, data = pdata, model = "random")
fixed_model <- plm(Income ~ Year + Education + Age, data = pdata, model = "within")
# Perform Hausman test
hausman_test <- phtest(fixed_model, random_model)
if (hausman_test$p.value <= 0.05) {
# Fixed Effect model if correlation exist
print("Fixed Effects")
return(fixed_model)
} else {
# Random Effect model if the correlation does not exists
print("Random Effects")
return(random_model)
}
}
})
# Step 4: Pool the results
pooled_results <- testEstimates(model_list)
# Return the pooled results summary
return(summary(pooled_results))
}
# Apply the analysis function to each set of imputed datasets
analyze_mitml_unbal_mcar_50 <- Analyze_mitml_Unbal(mitml_unbal_mcar_50)
analyze_mitml_unbal_mcar_30 <- Analyze_mitml_Unbal(mitml_unbal_mcar_30)
analyze_mitml_unbal_mcar_10 <- Analyze_mitml_Unbal(mitml_unbal_mcar_10)
analyze_mitml_unbal_mar_50 <- Analyze_mitml_Unbal(mitml_unbal_mar_50)
analyze_mitml_unbal_mar_30 <- Analyze_mitml_Unbal(mitml_unbal_mar_30)
analyze_mitml_unbal_mar_10 <- Analyze_mitml_Unbal(mitml_unbal_mar_10)
analyze_mitml_unbal_mnar_50 <- Analyze_mitml_Unbal(mitml_unbal_mnar_50)
analyze_mitml_unbal_mnar_30 <- Analyze_mitml_Unbal(mitml_unbal_mnar_30)
analyze_mitml_unbal_mnar_10 <- Analyze_mitml_Unbal(mitml_unbal_mnar_10)
EndTime_mitml <- Sys.time()  # Ending time
ExecutionTime_mitml <- EndTime - StartTime
print(ExecutionTime_mitml)
ExecutionTime_mitml <- EndTime_mitml - StartTime_mitml
print(ExecutionTime_mitml)
library(Amelia)
# Define the function for panel data conversion and imputation
Data_Imputation_Amelia <- function(data) {
# Convert the data to panel data using plm package
pdata <- pdata.frame(data, index = c("ID", "Year"))
pdata = pdata[c("ID", "Year", "Education", "Age", "Income")]
pdata$Year <- as.numeric(as.character(pdata$Year))
# Perform the imputation using Amelia
amelia_fit <- amelia(
pdata,
m = 3,
ts = "Year",
cs = "ID",
noms = "Education"
)
# Return the fitted Amelia object
return(amelia_fit)
}
amelia_bal_mcar_50 <- Data_Imputation_Amelia(balanced_panel_data_mcar_50)
amelia_bal_mcar_30 <- Data_Imputation_Amelia(balanced_panel_data_mcar_30)
amelia_bal_mcar_10 <- Data_Imputation_Amelia(balanced_panel_data_mcar_10)
amelia_bal_mar_50 <- Data_Imputation_Amelia(balanced_panel_data_mar_50)
amelia_bal_mar_30 <- Data_Imputation_Amelia(balanced_panel_data_mar_30)
amelia_bal_mar_10 <- Data_Imputation_Amelia(balanced_panel_data_mar_10)
amelia_bal_mnar_50 <- Data_Imputation_Amelia(balanced_panel_data_mnar_50)
amelia_bal_mnar_30 <- Data_Imputation_Amelia(balanced_panel_data_mnar_30)
amelia_bal_mnar_10 <- Data_Imputation_Amelia(balanced_panel_data_mnar_10)
amelia_unbal_mcar_50 <- Data_Imputation_Amelia(unbalanced_panel_data_mcar_50)
amelia_unbal_mcar_30 <- Data_Imputation_Amelia(unbalanced_panel_data_mcar_30)
amelia_unbal_mcar_10 <- Data_Imputation_Amelia(unbalanced_panel_data_mcar_10)
amelia_unbal_mar_50 <- Data_Imputation_Amelia(unbalanced_panel_data_mar_50)
amelia_unbal_mar_30 <- Data_Imputation_Amelia(unbalanced_panel_data_mar_30)
amelia_unbal_mar_10 <- Data_Imputation_Amelia(unbalanced_panel_data_mar_10)
amelia_unbal_mnar_50 <- Data_Imputation_Amelia(unbalanced_panel_data_mnar_50)
amelia_unbal_mnar_30 <- Data_Imputation_Amelia(unbalanced_panel_data_mnar_30)
amelia_unbal_mnar_10 <- Data_Imputation_Amelia(unbalanced_panel_data_mnar_10)
print(ExecutionTime_mitml)
StartTime_amelia <- Sys.time()  # Starting time
# Function to perform analysis on imputed data
Analyze_Amelia <- function(data) {
# Step 1: Perform data imputation
amelia_fit <- Data_Imputation_Amelia(data)
# Extract imputed datasets
imputed_list <- amelia_fit$imputations
# Step 2: Perform Breusch-Pagan test to check for a panel effect
breusch_pagan_results <- lapply(imputed_list, function(x) {
pdata <- pdata.frame(x, index = c("ID", "Year"))
bp_test <- plmtest(plm(Income ~ Year + Education + Age, data = pdata, model = "pooling"), type = "bp")
return(bp_test$p.value)
})
# Step 3: Based on Breusch-Pagan test, perform the appropriate regression
model_list <- lapply(1:length(imputed_list), function(i) {
pdata <- pdata.frame(imputed_list[[i]], index = c("ID", "Year"))
if (breusch_pagan_results[[i]] > 0.05) {
# Pooled OLS for no panel effect
return(plm(Income ~ Year + Education + Age, data = pdata, model = "pooling"))
} else {
# Hausman test if panel effect exists
random_model <- plm(Income ~ Year + Education + Age, data = pdata, model = "random")
fixed_model <- plm(Income ~ Year + Education + Age, data = pdata, model = "within")
# Perform Hausman test
hausman_test <- phtest(fixed_model, random_model)
if (hausman_test$p.value <= 0.05) {
# Use Fixed Effect model if correlation exists
print("Fixed Effects")
return(fixed_model)
} else {
# Random Effect model if correlation does not exists
print("Random Effects")
return(random_model)
}
}
})
# Step 4: Pool the results
pooled_results <- testEstimates(model_list)
# Return the pooled results summary
return(summary(pooled_results))
}
# Apply the function to each dataset and store results
analyze_amelia_bal_mcar_50 <- Analyze_Amelia(balanced_panel_data_mcar_50)
analyze_amelia_bal_mcar_30 <- Analyze_Amelia(balanced_panel_data_mcar_30)
analyze_amelia_bal_mcar_10 <- Analyze_Amelia(balanced_panel_data_mcar_10)
analyze_amelia_bal_mar_50 <- Analyze_Amelia(balanced_panel_data_mar_50)
analyze_amelia_bal_mar_30 <- Analyze_Amelia(balanced_panel_data_mar_30)
analyze_amelia_bal_mar_10 <- Analyze_Amelia(balanced_panel_data_mar_10)
analyze_amelia_bal_mnar_50 <- Analyze_Amelia(balanced_panel_data_mnar_50)
analyze_amelia_bal_mnar_30 <- Analyze_Amelia(balanced_panel_data_mnar_30)
analyze_amelia_bal_mnar_10 <- Analyze_Amelia(balanced_panel_data_mnar_10)
analyze_amelia_unbal_mcar_50 <- Analyze_Amelia(unbalanced_panel_data_mcar_50)
analyze_amelia_unbal_mcar_30 <- Analyze_Amelia(unbalanced_panel_data_mcar_30)
analyze_amelia_unbal_mcar_10 <- Analyze_Amelia(unbalanced_panel_data_mcar_10)
analyze_amelia_unbal_mar_50 <- Analyze_Amelia(unbalanced_panel_data_mar_50)
analyze_amelia_unbal_mar_30 <- Analyze_Amelia(unbalanced_panel_data_mar_30)
analyze_amelia_unbal_mar_10 <- Analyze_Amelia(unbalanced_panel_data_mar_10)
analyze_amelia_unbal_mnar_50 <- Analyze_Amelia(unbalanced_panel_data_mnar_50)
analyze_amelia_unbal_mnar_30 <- Analyze_Amelia(unbalanced_panel_data_mnar_30)
analyze_amelia_unbal_mnar_10 <- Analyze_Amelia(unbalanced_panel_data_mnar_10)
EndTime_amelia <- Sys.time()  # Ending time
ExecutionTime_amelia <- EndTime_amelia - StartTime_amelia
print(ExecutionTime_amelia)
StartTime_mice <- Sys.time()  # Starting time
Data_Imputation_mice <- function(data, m = 3, maxit = 20, method = 'pmm') {
# Select the necessary columns
data_temp <- data[c("Year", "Education", "Age", "Income")]
# Perform MICE imputation
mice_imp <- mice(data_temp, method = method, m = m, maxit = maxit)
# Return the imputation object
return(mice_imp)
}
# Apply imputation
mice_bal_mcar_50 <- Data_Imputation_mice(balanced_panel_data_mcar_50)
mice_bal_mcar_30 <- Data_Imputation_mice(balanced_panel_data_mcar_30)
mice_bal_mcar_10 <- Data_Imputation_mice(balanced_panel_data_mcar_10)
mice_bal_mar_50 <- Data_Imputation_mice(balanced_panel_data_mar_50)
mice_bal_mar_30 <- Data_Imputation_mice(balanced_panel_data_mar_30)
mice_bal_mar_10 <- Data_Imputation_mice(balanced_panel_data_mar_10)
mice_bal_mnar_50 <- Data_Imputation_mice(balanced_panel_data_mnar_50)
mice_bal_mnar_30 <- Data_Imputation_mice(balanced_panel_data_mnar_30)
mice_bal_mnar_10 <- Data_Imputation_mice(balanced_panel_data_mnar_10)
mice_unbal_mcar_50 <- Data_Imputation_mice(unbalanced_panel_data_mcar_50)
mice_unbal_mcar_30 <- Data_Imputation_mice(unbalanced_panel_data_mcar_30)
mice_unbal_mcar_10 <- Data_Imputation_mice(unbalanced_panel_data_mcar_10)
mice_unbal_mar_50 <- Data_Imputation_mice(unbalanced_panel_data_mar_50)
mice_unbal_mar_30 <- Data_Imputation_mice(unbalanced_panel_data_mar_30)
mice_unbal_mar_10 <- Data_Imputation_mice(unbalanced_panel_data_mar_10)
mice_unbal_mnar_50 <- Data_Imputation_mice(unbalanced_panel_data_mnar_50)
mice_unbal_mnar_30 <- Data_Imputation_mice(unbalanced_panel_data_mnar_30)
mice_unbal_mnar_10 <- Data_Imputation_mice(unbalanced_panel_data_mnar_10)
Analyze_mice <- function(mice_imp) {
# Fit the linear model
model <- with(mice_imp, lm(Income ~ Year + Education + Age))
# Pool the results
pooled_results <- pool(model)
# Return the summary of the pooled results
return(summary(pooled_results))
}
# Apply analysis
analyze_mice_bal_mcar_50 <- Analyze_mice(mice_bal_mcar_50)
analyze_mice_bal_mcar_30 <- Analyze_mice(mice_bal_mcar_30)
analyze_mice_bal_mcar_10 <- Analyze_mice(mice_bal_mcar_10)
analyze_mice_bal_mar_50 <- Analyze_mice(mice_bal_mar_50)
analyze_mice_bal_mar_30 <- Analyze_mice(mice_bal_mar_30)
analyze_mice_bal_mar_10 <- Analyze_mice(mice_bal_mar_10)
analyze_mice_bal_mnar_50 <- Analyze_mice(mice_bal_mnar_50)
analyze_mice_bal_mnar_30 <- Analyze_mice(mice_bal_mnar_30)
analyze_mice_bal_mnar_10 <- Analyze_mice(mice_bal_mnar_10)
analyze_mice_unbal_mcar_50 <- Analyze_mice(mice_unbal_mcar_50)
analyze_mice_unbal_mcar_30 <- Analyze_mice(mice_unbal_mcar_30)
analyze_mice_unbal_mcar_10 <- Analyze_mice(mice_unbal_mcar_10)
analyze_mice_unbal_mar_50 <- Analyze_mice(mice_unbal_mar_50)
analyze_mice_unbal_mar_30 <- Analyze_mice(mice_unbal_mar_30)
analyze_mice_unbal_mar_10 <- Analyze_mice(mice_unbal_mar_10)
analyze_mice_unbal_mnar_50 <- Analyze_mice(mice_unbal_mnar_50)
analyze_mice_unbal_mnar_30 <- Analyze_mice(mice_unbal_mnar_30)
analyze_mice_unbal_mnar_10 <- Analyze_mice(mice_unbal_mnar_10)
EndTime_mice <- Sys.time()  # Ending time
ExecutionTime_mice <- EndTime_mice - StartTime_mice
print(ExecutionTime_mice)
library(keras)
library(dplyr)
library(plm)
StartTime_LSTM <- Sys.time()  # Starting time
# Function to impute the data
Data_Imputation_LSTM <- function(data) {
# Temporarily filling the missing values in the Income column to take care of the missing values.
temp_data <- data %>%
mutate(Income = ifelse(is.na(Income), mean(Income, na.rm = TRUE), Income))
# Convert the column 'Education' to numeric using one-hot encoding
encoded_data <- temp_data %>%
mutate(Education = as.factor(Education)) %>%
select(Age, Income, Education)
encoded_data <- as.data.frame(model.matrix(~ Education - 1, data = encoded_data))
# Include other numeric columns (Age and Income)
input_data <- cbind(data$Age, data$Income, encoded_data)
# Convert the data to a 3D array, which is suitable for LSTM
train_array <- array(as.matrix(input_data), dim = c(nrow(input_data), 1, ncol(input_data)))
# Define LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(1, ncol(input_data)), return_sequences = FALSE) %>%
layer_dense(units = 1)
# Compile the model
model %>% compile(
loss = "mean_squared_error",
optimizer = optimizer_adam()  # Use the standard Adam optimizer
)
# Fit the model (training)
model %>% fit(
x = train_array,
y = temp_data$Income,  # Target is the Income column
epochs = 50,
batch_size = 32
)
# Predict values for the missing data
predicted_values <- model %>% predict(train_array)
# Replace missing values with the predicted values
data$Income[is.na(data$Income)] <- predicted_values[is.na(data$Income)]
# Return the imputed data
return(data)
}
# Apply the function to each dataset and store results
lstm_bal_mcar_50 <- Data_Imputation_LSTM(balanced_panel_data_mcar_50)
lstm_bal_mcar_30 <- Data_Imputation_LSTM(balanced_panel_data_mcar_30)
lstm_bal_mcar_10 <- Data_Imputation_LSTM(balanced_panel_data_mcar_10)
# Function to impute the data
Data_Imputation_LSTM <- function(data) {
# Temporarily filling the missing values in the Income column to take care of the missing values.
temp_data <- data %>%
mutate(Income = ifelse(is.na(Income), mean(Income, na.rm = TRUE), Income))
# Convert the column 'Education' to numeric using one-hot encoding
encoded_data <- temp_data %>%
mutate(Education = as.factor(Education)) %>%
select(Age, Income, Education)
encoded_data <- as.data.frame(model.matrix(~ Education - 1, data = encoded_data))
# Include other numeric columns (Age and Income)
input_data <- cbind(data$Age, data$Income, encoded_data)
# Convert the data to a 3D array, which is suitable for LSTM
train_array <- array(as.matrix(input_data), dim = c(nrow(input_data), 1, ncol(input_data)))
# Define LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 25, input_shape = c(1, ncol(input_data)), return_sequences = FALSE) %>%
layer_dense(units = 1)
# Compile the model
model %>% compile(
loss = "mean_squared_error",
optimizer = optimizer_adam()  # Use the standard Adam optimizer
)
# Fit the model (training)
model %>% fit(
x = train_array,
y = temp_data$Income,  # Target is the Income column
epochs = 50,
batch_size = 32
)
# Predict values for the missing data
predicted_values <- model %>% predict(train_array)
# Replace missing values with the predicted values
data$Income[is.na(data$Income)] <- predicted_values[is.na(data$Income)]
# Return the imputed data
return(data)
}
# Apply the function to each dataset and store results
lstm_bal_mcar_50 <- Data_Imputation_LSTM(balanced_panel_data_mcar_50)
# Function to impute the data
Data_Imputation_LSTM <- function(data) {
# Temporarily filling the missing values in the Income column to take care of the missing values.
temp_data <- data %>%
mutate(Income = ifelse(is.na(Income), mean(Income, na.rm = TRUE), Income))
# Convert the column 'Education' to numeric using one-hot encoding
encoded_data <- temp_data %>%
mutate(Education = as.factor(Education)) %>%
select(Age, Income, Education)
encoded_data <- as.data.frame(model.matrix(~ Education - 1, data = encoded_data))
# Include other numeric columns (Age and Income)
input_data <- cbind(data$Age, data$Income, encoded_data)
# Convert the data to a 3D array, which is suitable for LSTM
train_array <- array(as.matrix(input_data), dim = c(nrow(input_data), 1, ncol(input_data)))
# Define LSTM model
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(1, ncol(input_data)), return_sequences = FALSE) %>%
layer_dense(units = 1)
# Compile the model
model %>% compile(
loss = "mean_squared_error",
optimizer = optimizer_adam()  # Use the standard Adam optimizer
)
# Fit the model (training)
model %>% fit(
x = train_array,
y = temp_data$Income,  # Target is the Income column
epochs = 50,
batch_size = 32
)
# Predict values for the missing data
predicted_values <- model %>% predict(train_array)
# Replace missing values with the predicted values
data$Income[is.na(data$Income)] <- predicted_values[is.na(data$Income)]
# Return the imputed data
return(data)
}
StartTime_LSTM <- Sys.time()  # Starting time
# Function to analyse imputed data
Analyze_LSTM <- function(data) {
# Step 1: Perform data imputation
imputed_data <- Data_Imputation_LSTM(data)
# Convert to panel data
pdata <- pdata.frame(imputed_data, index = c("ID", "Year"))
# Step 2: Perform Breusch-Pagan test to check if panel effect exist
bp_test <- plmtest(plm(Income ~ Year + Education + Age, data = pdata, model = "pooling"), type = "bp")
# Step 3: Based on Breusch-Pagan test, perform the appropriate regression
if (bp_test$p.value > 0.05) {
# Pooled OLS for no panel effect
model <- plm(Income ~ Year + Education + Age, data = pdata, model = "pooling")
} else {
# Hausman test if panel effect exist
random_model <- plm(Income ~ Year + Education + Age, data = pdata, model = "random")
fixed_model <- plm(Income ~ Year + Education + Age, data = pdata, model = "within")
hausman_test <- phtest(fixed_model, random_model)
if (hausman_test$p.value <= 0.05) {
# Use Fixed effect model if correlation exists
model <- fixed_model
} else {
# Use Random Effect model if correlation does not exists
model <- random_model
}
}
# Step 4: Return the model summary
return(summary(model))
}
# Apply the function to each dataset and store results
analyze_lstm_bal_mcar_50 <- Analyze_LSTM(balanced_panel_data_mcar_50)
analyze_lstm_bal_mcar_30 <- Analyze_LSTM(balanced_panel_data_mcar_30)
analyze_lstm_bal_mcar_10 <- Analyze_LSTM(balanced_panel_data_mcar_10)
analyze_lstm_bal_mar_50 <- Analyze_LSTM(balanced_panel_data_mar_50)
analyze_lstm_bal_mar_30 <- Analyze_LSTM(balanced_panel_data_mar_30)
analyze_lstm_bal_mar_10 <- Analyze_LSTM(balanced_panel_data_mar_10)
analyze_lstm_bal_mnar_50 <- Analyze_LSTM(balanced_panel_data_mnar_50)
analyze_lstm_bal_mnar_30 <- Analyze_LSTM(balanced_panel_data_mnar_30)
analyze_lstm_bal_mnar_10 <- Analyze_LSTM(balanced_panel_data_mnar_10)
analyze_lstm_unbal_mcar_50 <- Analyze_LSTM(unbalanced_panel_data_mcar_50)
analyze_lstm_unbal_mcar_30 <- Analyze_LSTM(unbalanced_panel_data_mcar_30)
analyze_lstm_unbal_mcar_10 <- Analyze_LSTM(unbalanced_panel_data_mcar_10)
analyze_lstm_unbal_mar_50 <- Analyze_LSTM(unbalanced_panel_data_mar_50)
analyze_lstm_unbal_mar_30 <- Analyze_LSTM(unbalanced_panel_data_mar_30)
analyze_lstm_unbal_mar_10 <- Analyze_LSTM(unbalanced_panel_data_mar_10)
analyze_lstm_unbal_mnar_50 <- Analyze_LSTM(unbalanced_panel_data_mnar_50)
analyze_lstm_unbal_mnar_30 <- Analyze_LSTM(unbalanced_panel_data_mnar_30)
analyze_lstm_unbal_mnar_10 <- Analyze_LSTM(unbalanced_panel_data_mnar_10)
EndTime_LSTM <- Sys.time()  # Ending time
ExecutionTime_LSTM <- EndTime_LSTM - StartTime_LSTM
print(ExecutionTime_LSTM) # Time difference of  mins
?mice
packageVersion("mice")
packageVersion("mitml")
packageVersion("Amelia")
packageVersion("keras")
?`mitml-package`
data
View(data)
###################################
## Tipu Sultan
###################################
# Set Directory
setwd("/Users/tipusultan/Documents/GitHub/Imputation-of-Panel-Data")
########################
## Data
########################
library(dplyr)
library(readxl)
library(ggplot2)
# Load and clean the data
RawData <- readRDS("population.RDS")
RawData = data.frame(RawData)
data = RawData[c("id", "year","EF310", "EF44", "inc.ind")]
colnames(data) <- c("ID", "Year", "Education", "Age", "IndividualIncome")
summary(data)
# 'ID' column
count(data, ID)
# 'Year' column
count(data, Year)
# Keeping the data from 2013 to 2023
data <- subset(data, Year >= 2013 & Year <= 2023)
# 'Education' column - Highest general school degree
count(data, Education)
data$Education[is.na(data$Education)] <- 7
# Age
count(data, Age)
summary(data$Age)
# Density curve for Age
AgeDist <- ggplot(data, aes(x = Age)) +
geom_density() +
labs(title = "Density Curve of Age", x = "Age", y = "Density") +
theme_minimal()
AgeDist
# 'IndividualIncome' column: Income
summary(data$IndividualIncome)
count(data, IndividualIncome)
data$IndividualIncome[is.na(data$IndividualIncome)] <- 0
summary(data$IndividualIncome)
# Histogram for IndividualIncome
IndividualIncomeHist <- ggplot(data, aes(x = IndividualIncome)) +
geom_histogram(binwidth = 1000) +
labs(title = "Histogram of Individual Income", x = "Individual Income", y = "Frequency") +
theme_minimal()
IndividualIncomeHist
# Apply log transformation to IndividualIncome (adding 1 to avoid log(0))
data$LogIndividualIncome <- log(data$IndividualIncome + 1)
# Plot the histogram with the log-transformed IndividualIncome
IndividualIncomeLogHist <- ggplot(data, aes(x = LogIndividualIncome)) +
geom_histogram(binwidth = .10) +
labs(title = "Histogram of Logged IndividualIndividual", x = "Logged IndividualIncome", y = "Frequency")+
theme_minimal()
IndividualIncomeLogHist
sum(is.na(data)) #Total number of missing values
data <- data[c("ID", "Year", "Education", "Age", "LogIndividualIncome")]
colnames(data)[colnames(data) == "LogIndividualIncome"] <- "Income"
summary(data)
View(data)
View(balanced_panel_data)
View(balanced_panel_data)
analyze_lstm_bal_mcar_50
