set.seed(123)
x <- seq(0, 10, length.out = 100)
y <- sin(x) + rnorm(100, sd = 0.2)
# Train the SVR model
svm_model <- svm(y ~ x, data = data.frame(x = x, y = y), kernel = "radial")
# Make predictions
predictions <- predict(svm_model, data.frame(x = x))
# Plot the data and SVR fit
plot(x, y, main = "Support Vector Regression", xlab = "x", ylab = "y")
points(x, predictions, col = "red", pch = 20)
legend("topright", legend = c("Data", "SVR Fit"), col = c("black", "red"), pch = c(1, 20))
#################################################
# Generating the data and plot the clusters
#################################################
## Load necessary libraries
library(e1071)
library(ggplot2)
# Set seed for reproducibility
set.seed(1906)
# Generate data for the first cluster
cluster1 <- data.frame(
x = rnorm(50, mean = 2, sd = 0.5),
y = rnorm(50, mean = 2, sd = 0.5)
)
# Generate data for the second cluster
cluster2 <- data.frame(
x = rnorm(50, mean = 6, sd = 0.5),
y = rnorm(50, mean = 6, sd = 0.5)
)
# Combine the clusters into one dataset
data <- rbind(cluster1, cluster2)
# Add a label for the clusters
data$cluster <- as.factor(c(rep(1, 50), rep(2, 50)))
# View the first few rows of the dataset
head(data)
# Create a scatter plot of the data using ggplot2
ggplot(data, aes(x = x, y = y, color = cluster)) +
geom_point(size = 3) +
labs(title = "Scatter Plot of Two Clusters",
x = "X Axis",
y = "Y Axis",
color = "Cluster") +
theme_minimal()
##################################################################
# Adding the seperating single and multiple hyperplane
#################################################################
# Fit an SVM model
svm_model <- svm(cluster ~ ., data = data, kernel = "linear", scale = FALSE)
# Extract the coefficients and intercept
w <- t(svm_model$coefs) %*% svm_model$SV
b <- svm_model$rho
# Plot the data points
plot(data[, 1:2], col = as.integer(data$cluster), pch = 19, xlab = "x", ylab = "y")
abline(h = 0, v = 0, lty = 3)
# Add the support vectors to the plot
points(data[svm_model$index, 1:2], col = "purple", cex = 3, pch = 9)
# Main hyperplane
slope <- -w[1] / w[2]
intercept <- b / w[2]
abline(intercept, slope, col = "blue", lwd = 3)
# Function to plot additional hyperplanes
plot_hyperplanes <- function(slope, intercept, num_planes, step_size) {
for (i in seq(-num_planes, num_planes)) {
abline(intercept + i * step_size, slope, col = "red", lty = 2, lwd = 3)
}
}
# Add multiple hyperplanes
plot_hyperplanes(slope, intercept, num_planes = 5, step_size = 0.5)
library(scatterplot3d)
library(lattice)
# Define the coefficients of the plane equation
a <- 2
b <- -3
c <- 1
d <- 5
# Generate grid of points in the xy-plane
x <- seq(-10, 10, length.out = 100)
y <- seq(-10, 10, length.out = 100)
grid <- expand.grid(x = x, y = y)
# Calculate z-values for the plane
grid$z <- (d - a * grid$x - b * grid$y) / c
# Plot the 3D space
scatterplot3d(grid$x, grid$y, grid$z, main = "3D Space with 2D Plane", color = "blue", pch = 20, type = "b")
# Add a wireframe representation of the 2D plane to the plot
wireframe(z ~ x * y, data = grid, col = "blue", alpha = 0.5, add = TRUE)
##################################################################
# The Kernal Trick
#################################################################
# Load libraries
library(plotly)
# Function to generate intertwined clusters
generate_clusters <- function(n, center, sd) {
x <- rnorm(n, center[1], sd)
y <- rnorm(n, center[2], sd)
return(data.frame(x = x, y = y))
}
# Number of points in each cluster
n <- 50
# Generate two clusters
cluster1 <- generate_clusters(n, c(0, 0), 0.5)
cluster2 <- generate_clusters(n, c(1, 1), 0.5)
# Combine the clusters and add cluster information
cluster1$cluster <- 1
cluster2$cluster <- 2
data <- rbind(cluster1, cluster2)
# Calculate the mean for the entire dataset
mean_all <- colMeans(data[, 1:2])
# Calculate squared distances from mean for both clusters
data$z <- ((data$x - mean_all[1])^2 + (data$y - mean_all[2])^2)
# Add a scaler to the second cluster
data$z <- ifelse(data$cluster == 2, data$z + 10, data$z)
# Plot 2D graph
ggplot(data, aes(x = x, y = y, color = factor(cluster))) +
geom_point() +
theme_minimal() +
labs(title = "Intertwined Clusters", color = "Cluster") +
scale_color_manual(values = c("blue", "red"))
# Plot the data in 3D to show the separation
plot_ly(data, x = ~x, y = ~y, z = ~z, color = ~factor(cluster), colors = c("blue", "red")) %>%
add_markers() %>%
layout(scene = list(
xaxis = list(title = "X"),
yaxis = list(title = "Y"),
zaxis = list(title = "Z")),
title = "3D Plot of Separable Clusters"
)
# Set seed for reproducibility
set.seed(1906)
# Generate data for the first cluster
cluster1 <- data.frame(
x = rnorm(50, mean = 2, sd = 0.5),
y = rnorm(50, mean = 2, sd = 0.5)
)
# Generate data for the second cluster
cluster2 <- data.frame(
x = rnorm(50, mean = 6, sd = 0.5),
y = rnorm(50, mean = 6, sd = 0.5)
)
# Generate data for the third cluster
cluster3 <- data.frame(
x = rnorm(50, mean = 4, sd = 0.5),
y = rnorm(50, mean = 10, sd = 0.5)
)
# Combine the clusters into one dataset
data <- rbind(cluster1, cluster2, cluster3)
# Add a label for the clusters
data$cluster <- as.factor(c(rep(1, 50), rep(2, 50), rep(3, 50)))
# One-vs-Rest SVM training
models <- list()
for (i in 1:3) {
binary_labels <- as.factor(ifelse(data$cluster == i, 1, 0))
models[[i]] <- svm(x = data[, 1:2], y = binary_labels, kernel = "linear", scale = FALSE)
}
# Plot the data points
plot(data[, 1:2], col = as.integer(data$cluster), pch = 19, xlab = "x", ylab = "y")
abline(h = 0, v = 0, lty = 3)
# Add the support vectors to the plot
for (model in models) {
points(data[model$index, 1:2], col = "purple", cex = 3, pch = 9)
}
# Function to plot a single hyperplane
plot_hyperplane <- function(model, col = "blue", lwd = 3) {
w <- t(model$coefs) %*% model$SV
intercept <- model$rho
slope <- -w[1] / w[2]
intercept <- intercept / w[2]
abline(intercept, slope, col = col, lwd = lwd)
}
# Plot all separating hyperplanes
colors <- c("black", "red", "green")
plot_hyperplane(models[[1]], col = colors[1])
plot_hyperplane(models[[2]], col = colors[2])
plot_hyperplane(models[[3]], col = colors[3])
# Generate a new data point
new_point <- data.frame(
x = 3,
y = 6
)
# Classify the new data point using the trained SVM models
predictions <- sapply(models, function(model) predict(model, newdata = new_point))
# Identify the predicted cluster for the new point
predicted_cluster <- which.max(predictions)
# Visualize the new point along with the existing data and hyperplanes
plot(data[, 1:2], col = as.integer(data$cluster), pch = 19, xlab = "x", ylab = "y")
points(new_point$x, new_point$y, col = "blue", pch = 17)  # Plot the new point
abline(h = 0, v = 0, lty = 3)
# Add the support vectors to the plot
for (model in models) {
points(data[model$index, 1:2], col = "purple", cex = 3, pch = 9)
}
# Plot all separating hyperplanes
for (i in 1:3) {
plot_hyperplane(models[[i]], col = colors[i])
}
# Add a legend
legend("topright", legend = paste("Cluster", 1:3), col = colors, lty = 1, cex = 0.4)
# Function to calculate perpendicular distance and plot the line
plot_perpendicular_line <- function(model, point, col = "blue") {
w <- t(model$coefs) %*% model$SV
intercept <- model$rho / w[2]
slope <- -w[1] / w[2]
# Line equation: y = mx + b
# Perpendicular distance from point (x0, y0) to line y = mx + b
x0 <- point$x
y0 <- point$y
m <- slope
b <- intercept
d <- abs(m * x0 - y0 + b) / sqrt(m^2 + 1)
# Find the point on the line closest to (x0, y0)
x_closest <- (x0 + m * y0 - m * b) / (m^2 + 1)
y_closest <- (m * x0 + (m^2) * y0 - m^2 * b) / (m^2 + 1) + b
# Plot the line from the new point to the closest point on the hyperplane
segments(x0, y0, x_closest, y_closest, col = col, lwd = 3, lty = 2)
}
# Plot perpendicular lines from new point to each hyperplane
for (i in 1:3) {
plot_perpendicular_line(models[[i]], new_point, col = colors[i])
}
##########################################################
# Generate a new data point
new_point <- data.frame(
x = 6,
y = 4
)
# Classify the new data point using the trained SVM models
predictions <- sapply(models, function(model) predict(model, newdata = new_point))
# Identify the predicted cluster for the new point
predicted_cluster <- which.max(predictions)
# Visualize the new point along with the existing data and hyperplanes
plot(data[, 1:2], col = as.integer(data$cluster), pch = 19, xlab = "x", ylab = "y")
points(new_point$x, new_point$y, col = "blue", pch = 17)  # Plot the new point
abline(h = 0, v = 0, lty = 3)
# Add the support vectors to the plot
for (model in models) {
points(data[model$index, 1:2], col = "purple", cex = 3, pch = 9)
}
# Plot all separating hyperplanes
for (i in 1:3) {
plot_hyperplane(models[[i]], col = colors[i])
}
# Add a legend
legend("topright", legend = paste("Cluster", 1:3), col = colors, lty = 1, cex = 0.4)
# Function to calculate and plot the minimum distance line
plot_min_distance_line <- function(model, point, col = "blue") {
w <- t(model$coefs) %*% model$SV
b <- model$rho
norm_w <- sqrt(sum(w^2))
# Calculate the closest point on the hyperplane to the new point
x0 <- point$x
y0 <- point$y
d <- (w[1] * x0 + w[2] * y0 - b) / norm_w^2
x_closest <- x0 - d * w[1]
y_closest <- y0 - d * w[2]
# Plot the line from the new point to the closest point on the hyperplane
segments(x0, y0, x_closest, y_closest, col = col, lwd = 2, lty = 2)
}
# Plot minimum distance lines from the new point to each hyperplane
for (i in 1:3) {
plot_min_distance_line(models[[i]], new_point, col = colors[i])
}
# Generate a new data point
new_point <- data.frame(
x = 4.22,
y = 3.68
)
# Classify the new data point using the trained SVM models
predictions <- sapply(models, function(model) predict(model, newdata = new_point))
# Identify the predicted cluster for the new point
predicted_cluster <- which.max(predictions)
# Visualize the new point along with the existing data and hyperplanes
plot(data[, 1:2], col = as.integer(data$cluster), pch = 19, xlab = "x", ylab = "y")
points(new_point$x, new_point$y, col = "blue", pch = 17)  # Plot the new point
abline(h = 0, v = 0, lty = 3)
# Add the support vectors to the plot
for (model in models) {
points(data[model$index, 1:2], col = "purple", cex = 3, pch = 9)
}
# Plot all separating hyperplanes
for (i in 1:3) {
plot_hyperplane(models[[i]], col = colors[i])
}
# Add a legend
legend("topright", legend = paste("Cluster", 1:3), col = colors, lty = 1, cex = 0.4)
# Function to calculate and plot the minimum distance line
plot_min_distance_line <- function(model, point, col = "blue") {
w <- t(model$coefs) %*% model$SV
b <- model$rho
norm_w <- sqrt(sum(w^2))
# Calculate the closest point on the hyperplane to the new point
x0 <- point$x
y0 <- point$y
d <- (w[1] * x0 + w[2] * y0 - b) / norm_w^2
x_closest <- x0 - d * w[1]
y_closest <- y0 - d * w[2]
# Plot the line from the new point to the closest point on the hyperplane
segments(x0, y0, x_closest, y_closest, col = col, lwd = 2, lty = 2)
}
# Plot minimum distance lines from the new point to each hyperplane
for (i in 1:3) {
plot_min_distance_line(models[[i]], new_point, col = colors[i])
}
# Generate some example data
set.seed(123)
x <- seq(0, 10, length.out = 100)
y <- sin(x) + rnorm(100, sd = 0.2)
# Train the SVR model
svm_model <- svm(y ~ x, data = data.frame(x = x, y = y), kernel = "radial")
# Make predictions
predictions <- predict(svm_model, data.frame(x = x))
# Plot the data and SVR fit
plot(x, y, main = "Support Vector Regression", xlab = "x", ylab = "y")
points(x, predictions, col = "red", pch = 20)
legend("topright", legend = c("Data", "SVR Fit"), col = c("black", "red"), pch = c(1, 20))
legend("topright", legend = c("Data", "SVR Fit"), col = c("black", "red"), pch = c(1, 20), cex = .4)
# Generate some example data
set.seed(123)
x <- seq(0, 10, length.out = 100)
y <- sin(x) + rnorm(100, sd = 0.2)
# Train the SVR model
svm_model <- svm(y ~ x, data = data.frame(x = x, y = y), kernel = "radial")
# Make predictions
predictions <- predict(svm_model, data.frame(x = x))
# Plot the data and SVR fit
plot(x, y, main = "Support Vector Regression", xlab = "x", ylab = "y")
points(x, predictions, col = "red", pch = 20)
legend("topright", legend = c("Data", "SVR Fit"), col = c("black", "red"), pch = c(1, 20), cex = .6)
# install.packages("dbscan")
library(dbscan)
# Generate sample data
set.seed(1830)
data <- cbind(x = rnorm(100, mean = 0, sd = 1),
y = rnorm(100, mean = 0, sd = 1))
# Run DBSCAN algorithm
db <- dbscan(data, eps = 0.5, minPts = 5)
# Plot the clusters
plot(data, col = db$cluster + 1, pch = 19, main = "DBSCAN Clustering")
legend("topleft", legend = unique(db$cluster),
col = 1:length(unique(db$cluster)), pch = 19,
title = "Cluster")
# Install necessary packages
install.packages("gganimate")
install.packages("dplyr")
library(dbscan)
library(ggplot2)
library(gganimate)
library(dplyr)
# Generate sample data
set.seed(1830)
data <- data.frame(
x = rnorm(100, mean = 0, sd = 1),
y = rnorm(100, mean = 0, sd = 1)
)
# Parameters for DBSCAN
eps <- 0.5
minPts <- 5
# Function to perform DBSCAN step-by-step
dbscan_step_by_step <- function(data, eps, minPts) {
n <- nrow(data)
data$cluster <- -1  # initialize all labels as noise (-1)
data$step <- 0
step <- 1
for (i in 1:n) {
neighbors <- which(dist(data) <= eps, arr.ind = TRUE)[,1]
if (length(neighbors) >= minPts) {
data$cluster[neighbors] <- step
data$step[neighbors] <- step
step <- step + 1
}
}
return(data)
}
# Perform step-by-step DBSCAN
data_step_by_step <- dbscan_step_by_step(data, eps, minPts)
library(dbscan)
library(ggplot2)
library(gganimate)
library(dplyr)
# Generate sample data
set.seed(1830)
data <- data.frame(
x = rnorm(100, mean = 0, sd = 1),
y = rnorm(100, mean = 0, sd = 1)
)
# Parameters for DBSCAN
eps <- 0.5
minPts <- 5
# Function to perform DBSCAN step-by-step
dbscan_step_by_step <- function(data, eps, minPts) {
n <- nrow(data)
data$cluster <- -1  # initialize all labels as noise (-1)
data$step <- 0
step <- 1
distances <- as.matrix(dist(data[, c("x", "y")]))
for (i in 1:n) {
if (data$cluster[i] == -1) {  # process only unvisited points
neighbors <- which(distances[i, ] <= eps)
if (length(neighbors) >= minPts) {
data$cluster[neighbors] <- step
data$step[neighbors] <- step
step <- step + 1
}
}
}
return(data)
}
# Perform step-by-step DBSCAN
data_step_by_step <- dbscan_step_by_step(data, eps, minPts)
# Create the animation
p <- ggplot(data_step_by_step, aes(x = x, y = y, color = factor(cluster))) +
geom_point(size = 3) +
labs(title = 'DBSCAN Clustering Animation: Step {closest_state}', color = 'Cluster') +
theme_minimal() +
transition_states(step, transition_length = 1, state_length = 1) +
ease_aes('linear')
# Save the animation
animate(p, renderer = gifski_renderer("dbscan_animation.gif"))
########################
## Data
########################
# Load the data
data <- readRDS("population.RDS")
# Set Directory
setwd("/Users/tipusultan/Documents/GitHub/Imputation-of-Panel-Data")
########################
## Data
########################
# Load the data
data <- readRDS("population.RDS")
View(data)
head(data)
print(data)
data = data.frame(data)
data
head(data)
# Load required library
library(plm)
# Set parameters
n <- 100  # Number of individuals
T <- 5    # Number of time periods
# Generate individual and time indices
id <- rep(1:n, each = T)
time <- rep(1:T, times = n)
# Simulate individual-specific effects
alpha_i <- rnorm(n, mean = 0, sd = 1)
# Simulate time-specific effects
gamma_t <- rnorm(T, mean = 0, sd = 1)
# Generate random error term
error <- rnorm(n*T, mean = 0, sd = 1)
# Simulate balanced panel data
data_balanced <- data.frame(
id = id,
time = time,
x1 = rnorm(n*T),
x2 = rnorm(n*T),
y = 1 + 2 * rnorm(n*T) + 3 * rnorm(n*T) + alpha_i[id] + gamma_t[time] + error
)
# Convert to panel data object
balanced_panel_data <- pdata.frame(data_balanced, index = c("id", "time"))
# View the first few rows of the balanced panel data
head(balanced_panel_data)
head(data)
data$EF413
head(data)
data$EF44
head(data)
data$EF619
data
data = data[c("id", "year", "EF44", "EF619", "EF413")]
View(data)
data = na.omit(data[c("id", "year", "EF44", "EF619", "EF413")])
View(data)
library(reshape2)
pivot_table <- dcast(data, Name ~ Subject, value.var = "Score")
summary(data)
library(tidyr)
library(dplyr)
# Reshape the data to long format
long_df <- df %>%
pivot_longer(cols = starts_with("EF"), names_to = "variable", values_to = "value")
# Count the number of unique values for each variable
unique_counts <- df %>%
summarise(across(everything(), ~ n_distinct(.)))
# View the unique counts
print(unique_counts)
summary(data)
# Count unique values in the 'id' column
count(data, id)
# Count unique values in the 'year' column
count(data, year)
# Count unique values in the 'EF44' column
count(data, EF44)
# Count unique values in the 'EF619' column
count(data, EF619)
# Count unique values in the 'EF413' column
count(data, EF413)
data
# Count unique values in the 'EF619' column
count(data, EF619)
pivot_table <- pivot_wider(data, id_cols = id, names_from = year, values_from = id)
# Print pivot table
print(pivot_table)
library(tidyr)
pivot_table <- pivot_wider(data, id_cols = id, names_from = year, values_from = id)
# Create pivot table for id and year
pivot_table <- pivot_wider(data, names_from = year, values_from = id)
# Print pivot table
print(pivot_table)
# Count unique values in the 'id' column
count(data, id)
length(data)
length(data$id)
length(data$year)
